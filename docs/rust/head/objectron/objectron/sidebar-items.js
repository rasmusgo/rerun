window.SIDEBAR_ITEMS = {"mod":[["ar_blend_shape_map","Nested message and enum types in `ARBlendShapeMap`."],["ar_camera","Nested message and enum types in `ARCamera`."],["ar_face_geometry","Nested message and enum types in `ARFaceGeometry`."],["ar_light_estimate","Nested message and enum types in `ARLightEstimate`."],["ar_mesh_geometry","Nested message and enum types in `ARMeshGeometry`."],["ar_plane_anchor","Nested message and enum types in `ARPlaneAnchor`."],["ar_plane_geometry","Nested message and enum types in `ARPlaneGeometry`."],["ar_point_cloud","Nested message and enum types in `ARPointCloud`."],["av_depth_data","Nested message and enum types in `AVDepthData`."],["cm_calibrated_magnetic_field","Nested message and enum types in `CMCalibratedMagneticField`."],["cm_device_motion","Nested message and enum types in `CMDeviceMotion`."],["object","Nested message and enum types in `Object`."]],"struct":[["AnnotatedKeyPoint",""],["ArBlendShapeMap","Contains a list of blend shape entries wherein each item maps a specific blend shape location to its associated coefficient."],["ArCamera","Information about the camera position and imaging characteristics for a captured video frame."],["ArFaceAnchor","Information about the pose, topology, and expression of a detected face."],["ArFaceGeometry","Container for a 3D mesh describing face topology."],["ArFrame","Video image and face position tracking information."],["ArLightEstimate","Estimated scene lighting information associated with a captured video frame."],["ArMeshAnchor","A subdividision of the reconstructed, real-world scene surrounding the user."],["ArMeshData","Container object for mesh data of real-world scene surrounding the user. Even though each ARFrame may have a set of ARMeshAnchors associated with it, only a single frameâ€™s worth of mesh data is written separately at the end of each recording due to concerns regarding latency and memory bloat."],["ArMeshGeometry","Mesh geometry data stored in an array-based format."],["ArPlaneAnchor","Information about the position and orientation of a real-world flat surface."],["ArPlaneGeometry","Container for a 3D mesh."],["ArPointCloud","A collection of points in the world coordinate space."],["AvCameraCalibrationData","Info about the camera characteristics used to capture images and depth data."],["AvDepthData","Container for depth data information."],["CmAccelerometerData","A sample of raw accelerometer data."],["CmCalibratedMagneticField","Represents calibrated magnetic field data and accuracy estimate of it."],["CmDeviceMotion","A sample of device motion data. Encapsulates measurements of the attitude, rotation rate, magnetic field, and acceleration of the device. Core Media applies different algorithms of bias-reduction and stabilization to rotation rate, magnetic field and acceleration values. For raw values check correspondent fields in CMMotionManagerSnapshot object."],["CmGyroData","A sample of raw gyroscope data."],["CmMagnetometerData","A sample of raw magnetometer data."],["CmMotionManagerSnapshot","Contains most recent snapshots of device motion data"],["CmVector","A 3D vector"],["Edge","The edge connecting two keypoints together"],["FrameAnnotation",""],["KeyPoint",""],["NormalizedPoint2D","Projection of a 3D point on an image, and its metric depth."],["Object",""],["ObjectAnnotation",""],["Point3D","The 3D point in the camera coordinate system, the scales are in meters."],["Sequence","The sequence protocol contains the annotation data for the entire video clip."],["Skeleton","The skeleton template for different objects (e.g. humans, chairs, hands, etc) The annotation tool reads the skeleton template dictionary."],["Skeletons","The list of all the modeled skeletons in our library. These models can be objects (chairs, desks, etc), humans (full pose, hands, faces, etc), or box. We can have multiple skeletons in the same file."]]};